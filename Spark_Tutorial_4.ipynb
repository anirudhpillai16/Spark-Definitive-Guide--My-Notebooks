{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7 and 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-3.0.0-preview2-bin-hadoop2.7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"inferSchema\",\"true\")\\\n",
    ".load(\"C:/Users/pilla/Documents/Spark-The-Definitive-Guide-master/Spark-The-Definitive-Guide-master/data/retail-data/by-day/*.csv\")\\\n",
    ".coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3804|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\",0.05)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First and Last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                  23084|                 22168|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"),last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min and Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min,max\n",
    "df.select(min(\"Quantity\"),max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum Distinct\n",
    "\n",
    "sum distinct set of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|    avg_purchase|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, expr\n",
    "\n",
    "df.select(\n",
    "count(\"Quantity\").alias(\"total_transactions\"),\n",
    "sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "avg(\"Quantity\").alias(\"avg_purchase\"),\n",
    "expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    ".selectExpr(\"total_purchases/total_transactions\",\"avg_purchase\",\"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance and Standard Deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+---------------------+--------------------+\n",
      "|var_pop(Quantity)|var_samp(Quantity)|stddev_samp(Quantity)|stddev_pop(Quantity)|\n",
      "+-----------------+------------------+---------------------+--------------------+\n",
      "|47559.30364660885| 47559.39140929855|    218.0811578502337|   218.0809566344775|\n",
      "+-----------------+------------------+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop,stddev_pop\n",
    "from pyspark.sql.functions import var_samp,stddev_samp\n",
    "df.select(var_pop(\"Quantity\"),var_samp(\"Quantity\"),\n",
    "         stddev_samp(\"Quantity\"),stddev_pop(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|skewness(Quantity)|kurtosis(Quantity)|\n",
      "+------------------+------------------+\n",
      "|-0.264075576105286|119768.05495534562|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InVoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085617365E-4|             1052.7280543863135|            1052.7260778702093|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr,covar_pop,covar_samp\n",
    "df.select(corr(\"InvoiceNo\",\"Quantity\"), covar_samp(\"InVoiceNo\",\"Quantity\"), covar_pop(\"InvoiceNo\",\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating to Complex Types (Quite Useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+\n",
      "|collect_list(Country)|collect_set(Country)|\n",
      "+---------------------+--------------------+\n",
      "| [United Kingdom, ...|[Portugal, Italy,...|\n",
      "+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, collect_set\n",
    "df.agg(collect_list(\"Country\"), collect_set(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   574966|   8|              8|\n",
      "|   575091|  38|             38|\n",
      "|   578057|  28|             28|\n",
      "|   537252|   1|              1|\n",
      "|   578459|   8|              8|\n",
      "|  C578132|   1|              1|\n",
      "|   578292|  72|             72|\n",
      "|   576112|  20|             20|\n",
      "|   577022|  38|             38|\n",
      "|   574592|   8|              8|\n",
      "|  C576393|   2|              2|\n",
      "|   577511|  46|             46|\n",
      "|   577541|  21|             21|\n",
      "|   580739|   2|              2|\n",
      "|   580906|   4|              4|\n",
      "|   573726|   1|              1|\n",
      "|   575671|  20|             20|\n",
      "|   570264|   1|              1|\n",
      "|   570281|   3|              3|\n",
      "|   569823|  69|             69|\n",
      "+---------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "count(\"Quantity\").alias(\"quan\"),\n",
    "expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   574966|               6.0|   3.640054944640259|\n",
      "|   575091|11.552631578947368|   5.008925551458656|\n",
      "|   578057| 4.607142857142857|   8.755974636597271|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   578459|              28.0|                26.0|\n",
      "|  C578132|              -1.0|                 0.0|\n",
      "|   578292| 5.902777777777778|   8.759375488618884|\n",
      "|   576112|              10.9|  7.4959989327640635|\n",
      "|   577022| 5.131578947368421|   2.903455768848916|\n",
      "|   574592|              7.25|  4.4651427748729375|\n",
      "|  C576393|              -3.5|                 2.5|\n",
      "|   577511|3.1739130434782608|  5.4025128928727195|\n",
      "|   577541| 9.333333333333334|    9.18245393767158|\n",
      "|   580739|               2.5|                 0.5|\n",
      "|   580906|              27.0|  13.076696830622021|\n",
      "|   573726|             -67.0|                 0.0|\n",
      "|   575671|             16.65|   12.14197265686264|\n",
      "|   570264|             -22.0|                 0.0|\n",
      "|   570281|              48.0|                 0.0|\n",
      "|   569823|1.4782608695652173|  0.9869980199409517|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,to_date\n",
    "dfWithDate = df.withColumn(\"date\",to_date(col(\"InvoiceDate\")))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "windowSpec = Window\\\n",
    ".partitionBy(\"CustomerId\",\"date\")\\\n",
    ".orderBy(desc(\"Quantity\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max purchasing Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|   12395.0|2011-03-23|     100|           1|                1|                100|\n",
      "|   12395.0|2011-03-23|     100|           1|                1|                100|\n",
      "|   12395.0|2011-03-23|      48|           3|                2|                100|\n",
      "|   12395.0|2011-03-23|      20|           4|                3|                100|\n",
      "|   12395.0|2011-03-23|      12|           5|                4|                100|\n",
      "|   12395.0|2011-03-23|      12|           5|                4|                100|\n",
      "|   12395.0|2011-03-23|      10|           7|                5|                100|\n",
      "|   12395.0|2011-03-23|      10|           7|                5|                100|\n",
      "|   12395.0|2011-03-23|       6|           9|                6|                100|\n",
      "|   12395.0|2011-03-23|       3|          10|                7|                100|\n",
      "|   12451.0|2011-07-15|    -120|           1|                1|               -120|\n",
      "|   12457.0|2011-08-02|      -5|           1|                1|                 -5|\n",
      "|   12633.0|2011-02-25|     200|           1|                1|                200|\n",
      "|   12633.0|2011-02-25|      36|           2|                2|                200|\n",
      "|   12633.0|2011-02-25|      25|           3|                3|                200|\n",
      "|   12633.0|2011-02-25|      24|           4|                4|                200|\n",
      "|   12633.0|2011-02-25|      24|           4|                4|                200|\n",
      "|   12633.0|2011-02-25|      24|           4|                4|                200|\n",
      "|   12633.0|2011-02-25|      24|           4|                4|                200|\n",
      "|   12633.0|2011-02-25|      24|           4|                4|                200|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    ".select(\n",
    "col(\"CustomerId\"),\n",
    "col(\"date\"),\n",
    "col(\"Quantity\"),\n",
    "purchaseRank.alias(\"quantityRank\"),\n",
    "purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|   18287.0|    85173|           48|\n",
      "|   18287.0|   85040A|           48|\n",
      "|   18287.0|   85039B|          120|\n",
      "|   18287.0|   85039A|           96|\n",
      "|   18287.0|    84920|            4|\n",
      "|   18287.0|    84584|            6|\n",
      "|   18287.0|   84507C|            6|\n",
      "|   18287.0|   72351B|           24|\n",
      "|   18287.0|   72351A|           24|\n",
      "|   18287.0|   72349B|           60|\n",
      "|   18287.0|    47422|           24|\n",
      "|   18287.0|    47421|           48|\n",
      "|   18287.0|    35967|           36|\n",
      "|   18287.0|    23445|           20|\n",
      "|   18287.0|    23378|           24|\n",
      "|   18287.0|    23376|           48|\n",
      "|   18287.0|    23310|           36|\n",
      "|   18287.0|    23274|           12|\n",
      "|   18287.0|    23272|           12|\n",
      "|   18287.0|    23269|           36|\n",
      "+----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY CustomerId,stockCode GROUPING SETS((CustomerId,stockCode))\n",
    "ORDER BY CustomerId DESC, stockCode DESC\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roll Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      null|          null|       5176450|\n",
      "|2010-12-01|          null|         26814|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|United Kingdom|         23949|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-01|   Netherlands|            97|\n",
      "|2010-12-01|       Germany|           117|\n",
      "|2010-12-02|       Germany|           146|\n",
      "|2010-12-02|          EIRE|             4|\n",
      "|2010-12-02|          null|         21023|\n",
      "|2010-12-02|United Kingdom|         20873|\n",
      "|2010-12-03|        Poland|           140|\n",
      "|2010-12-03|        France|           239|\n",
      "|2010-12-03|       Germany|           170|\n",
      "|2010-12-03|      Portugal|            65|\n",
      "|2010-12-03|         Spain|           400|\n",
      "|2010-12-03|   Switzerland|           110|\n",
      "|2010-12-03|       Belgium|           528|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"date\",\"Country\").agg(sum(\"Quantity\"))\\\n",
    ".selectExpr(\"date\",\"Country\",\"`sum(Quantity)` as total_quantity\")\\\n",
    ".orderBy(\"date\")\n",
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+\n",
      "|Date|             Country|sum(Quantity)|\n",
      "+----+--------------------+-------------+\n",
      "|null|               Japan|        25218|\n",
      "|null|            Portugal|        16180|\n",
      "|null|           Australia|        83653|\n",
      "|null|             Germany|       117448|\n",
      "|null|                 RSA|          352|\n",
      "|null|              Cyprus|         6317|\n",
      "|null|           Hong Kong|         4769|\n",
      "|null|         Unspecified|         3300|\n",
      "|null|           Singapore|         5234|\n",
      "|null|                null|      5176450|\n",
      "|null|     Channel Islands|         9479|\n",
      "|null|             Finland|        10666|\n",
      "|null|             Denmark|         8188|\n",
      "|null|               Spain|        26824|\n",
      "|null|             Lebanon|          386|\n",
      "|null|  European Community|          497|\n",
      "|null|United Arab Emirates|          982|\n",
      "|null|              Norway|        19247|\n",
      "|null|      Czech Republic|          592|\n",
      "|null|                 USA|         1034|\n",
      "+----+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNoNull.cube(\"date\",\"Country\").agg(sum(col(\"Quantity\")))\\\n",
    ".select(\"Date\",\"Country\",\"sum(Quantity)\").orderBy(\"date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "    (0,\"Bill Chambers\",0,[100]),\n",
    "    (1,\"Matet Zaharia\",1,[500,200,100]),\n",
    "    (2,\"Michael Armbrust\",1,[250,100])])\\\n",
    ".toDF(\"id\",\"name\",\"graduate_program\",\"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0,\"Masters\",\"School of Information\",\"UC Berkely\"),\n",
    "    (2,\"Masters\",\"EECS\",\"UC Berkeley\"),\n",
    "    (1,\"Phd\",\"EECS\",\"UC Berkeley\")])\\\n",
    ".toDF(\"id\",\"degree\",\"department\",\"school\")\n",
    "sparkStatus = spark.createDataFrame([\n",
    "    (500,\"Vice President\"),\n",
    "    (250,\"PMC Member\"),\n",
    "    (100,\"Contributor\")])\\\n",
    ".toDF(\"id\",\"status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+\n",
      "| id|            name|graduate_program|   spark_status|\n",
      "+---+----------------+----------------+---------------+\n",
      "|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Matet Zaharia|               1|[500, 200, 100]|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...| UC Berkely|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|    Phd|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...| UC Berkely|\n",
      "|  1|   Matet Zaharia|               1|[500, 200, 100]|  1|    Phd|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|    Phd|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinExpression = person[\"graduate_program\"] == graduateProgram[\"id\"]\n",
    "person.join(graduateProgram,joinExpression,\"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...| UC Berkely|\n",
      "|   1|   Matet Zaharia|               1|[500, 200, 100]|  1|    Phd|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|    Phd|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduateProgram, joinExpression, \"outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...| UC Berkely|   0|   Bill Chambers|               0|          [100]|\n",
      "|  1|    Phd|                EECS|UC Berkeley|   1|   Matet Zaharia|               1|[500, 200, 100]|\n",
      "|  1|    Phd|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram.join(person, joinExpression, \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Right Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...| UC Berkely|\n",
      "|   1|   Matet Zaharia|               1|[500, 200, 100]|  1|    Phd|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|    Phd|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduateProgram, joinExpression, \"right_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left Semi Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    "    (0,\"Masters\",\"Duplicated Row\",\"Duplicated School\")]))\n",
    "gradProgram2.createOrReplaceTempView(\"gradProgram2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------------+\n",
      "| id| degree|          department|           school|\n",
      "+---+-------+--------------------+-----------------+\n",
      "|  0|Masters|School of Informa...|       UC Berkely|\n",
      "|  1|    Phd|                EECS|      UC Berkeley|\n",
      "|  0|Masters|      Duplicated Row|Duplicated School|\n",
      "+---+-------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exprs = gradProgram2[\"id\"]== person[\"graduate_program\"]\n",
    "gradProgram2.join(person, exprs, \"left_semi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left Anti Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gradProgram2.join(person, exprs, \"left_anti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...| UC Berkely|   Bill Chambers|               0|          [100]|\n",
      "|  1|    Phd|                EECS|UC Berkeley|   Matet Zaharia|               1|[500, 200, 100]|\n",
      "|  2|Masters|                EECS|UC Berkeley|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+-------+--------------------+-----------+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "spark.sql(\"\"\"SELECT * FROM graduateProgram NATURAL JOIN person\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school| id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...| UC Berkely|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|    Phd|                EECS|UC Berkeley|  1|   Matet Zaharia|               1|[500, 200, 100]|\n",
      "|  1|    Phd|                EECS|UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram.join(person, joinExpression, \"cross\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...| UC Berkely|\n",
      "|  0|   Bill Chambers|               0|          [100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  1|    Phd|                EECS|UC Berkeley|\n",
      "|  1|   Matet Zaharia|               1|[500, 200, 100]|  0|Masters|School of Informa...| UC Berkely|\n",
      "|  1|   Matet Zaharia|               1|[500, 200, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|   Matet Zaharia|               1|[500, 200, 100]|  1|    Phd|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  0|Masters|School of Informa...| UC Berkely|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|    Phd|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.crossJoin(graduateProgram).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable for cross join \"spark.sql.crossJoin.enable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join on Complex Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|personId|            name|graduate_program|   spark_status| id|        status|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|       0|   Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|       1|   Matet Zaharia|               1|[500, 200, 100]|500|Vice President|\n",
      "|       1|   Matet Zaharia|               1|[500, 200, 100]|100|   Contributor|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "person.withColumnRenamed(\"id\",\"personId\")\\\n",
    ".join(sparkStatus, expr(\"array_contains(spark_status,id)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Duplicate Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|graduate_program|\n",
      "+----------------+\n",
      "|               0|\n",
      "|               1|\n",
      "|               1|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduateProgram, joinExpression).select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
